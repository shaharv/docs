# Glow Compiler Overview (Slides)

## The Glow Compiler

- Glow is a retargetable neural network compiler, open sourced by Facebook in 2018

- Glow stands for “Graph LOWering”

- At the heart of Glow are two IR levels – the High Level IR and the Low Level IR - allowing for high level graph optimizations and low level instruction based optimizations, respectively

- Target specific optimizations are performed by the respective Backend (which is free to introduce additional IR levels)

- Supports JIT and AOT compilation modes, in addition to the Glow interpreter

- Supports loading ONNX, pytorch and Caffe2 models (through ONNX conversion)

## Glow Intermediate Representations

- Similar to traditional C/C++ compilers, Glow introduces several IR levels

- Both IR forms are strongly typed – tensor dimensions and types provided at compile time

- High Level IR – dataflow node-based graph representation:
  - Classic optimizations (CSE, DCE)
  - NN and Linear Algebra optimizations (merge concat/transpose nodes, identities to reduce tensor dims)
  - Quantization

- Low Level IR – Instruction based representation:

- Memory optimizations (DSE and dead allocation removal, shorten allocation life ranges, buffer sharing)

- Peephole optimizations

## Glow High Level IR

- Structured as a module containing functions, each containing multiple nodes

- Operator nodes – Convolution, MatrixMultiply, MaxPool, etc.

- Storage nodes:
  - Constant nodes – concrete compile time known tensors (e.g. weights)
  - Placeholder nodes – tensor variables

## Glow High Level IR Life Cycle

- Constructed by a direct translation of the input model
  - Each operator to one or more IR nodes
  
- The generated IR is optimized by the Graph Optimizer

- Then, Linear Algebra lowering is performed:
  - Saves the need to implement every single operator
  - Backends can exclude or defer operators from being lowered (shouldLower, transformPostLowering methods)

- Then, additional graph optimizations are performed

## Glow Low Level IR

- Generated by the IRGen phase

- Each IR function consists of two sections:
  - declare – global memory regions
  - program – list of instructions

- Operands are annotated with @in, @out, @inout

- Function local memory is denoted by alloc

- Instructions could have additional attributes

- Following IRGen, the LL IR is optimized as well

## Code Generation

- The backend receives the low level IR

- Concrete memory buffers are allocated

- The IR is compiled into LLVM-IR

- Optimized by the LLVM optimizer
  - Powerful vectorization
  - IPO - Linked with standard library LLVM bitcode

- Target code is generated

## IR/CG Life Cycle

![Glow IR Levels](https://github.com/shaharv/glow/blob/master/docs/3LevelIR.png)

## Glow Runtime

- Supports partitioning, queueing and running models across multiple devices

![Glow Runtime](https://github.com/shaharv/glow/blob/master/docs/glow_runtime.svg)

## Links

- Glow: Graph Lowering Compiler Techniques for Neural Networks  
  https://arxiv.org/pdf/1805.00907.pdf

- Glow talk at 2018 LLVM Developers’ Meeting  
  https://www.youtube.com/watch?v=cTz7c5dn5Gc
